model_path: "meta-llama/Llama-3.1-8B-Instruct"
quantization_bits: null
device: "cuda:0"
cache_dir: "/home/can/models/"
