model:
  model_path: allenai/Llama-3.1-Tulu-3-8B-SFT
  quantization_bits: null
  device: cuda
  cache_dir: /home/can/models/
prompt_injection_location: thought_prefix
use_openai_embeddings: true
load_fname: null
backend: transformers
device: cuda
vllm:
  tensor_parallel_size: 1
  gpu_memory_utilization: 0.9
  max_model_len: null
crawler:
  temperature: 0.6
  num_samples_per_topic: 1
  num_crawl_steps: 2
  generation_batch_size: 10
  max_topic_string_length: 200
  max_context_tokens: 500
  max_generated_tokens: 200
  max_extracted_topics_per_generation: 10
  max_crawl_topics: 10000
  tokenization_template: chat
  do_filter_refusals: true
  do_force_thought_skip: false
  prompt_languages:
  - english
  refusal_max_new_tokens: 25
verbose: true
